# -*- coding: utf-8 -*-
"""chatbot

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T5xgczYDtdKRmWs5nQfPobCn5WwbTEQc
"""







pip install transformers torch accelerate beautifulsoup4 requests

# Import required libraries
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import requests
from bs4 import BeautifulSoup

# Set the random seed for reproducibility
torch.manual_seed(0)

# Load the required model and tokenizer
model_name = "microsoft/Phi-3-mini-4k-instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto", trust_remote_code=True).to('cuda')
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Set pad token if not set
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Create the pipeline
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=0  # Ensure the pipeline uses GPU
)

# Define generation arguments
generation_args = {
    "max_new_tokens": 100,
    "return_full_text": False,
    "temperature": 0.7,
    "do_sample": True,
    "top_k": 50,
    "top_p": 0.9,
    "repetition_penalty": 1.2,  # Penalty for repeated tokens
}

# Memory to store conversation context
conversation_history = []

# Function to fetch information from Wikipedia
def fetch_wikipedia_info(query):
    search_url = f"https://en.wikipedia.org/wiki/{query.replace(' ', '_')}"
    response = requests.get(search_url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        paragraphs = soup.find_all('p')
        intro_text = ' '.join([para.get_text() for para in paragraphs[:3]])  # Get the first 3 paragraphs
        return intro_text
    else:
        return "I'm sorry, I couldn't fetch information from Wikipedia at the moment."

# Define the chatbot function with the pipeline
def chatbot():
    print("Welcome to the Phi-3-mini-4k-instruct Chatbot! Type 'exit' to stop.")
    global conversation_history
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'exit':
            break

        # Handle specific greeting input
        if user_input.lower() == 'hello':
            response = "Hello! How can I assist you today?"
        elif user_input.lower().startswith('tell me about'):
            query = user_input.lower().replace('tell me about', '').strip()
            response = fetch_wikipedia_info(query)
        else:
            prompt = f"You are a helpful assistant.\n\n{user_input}\n\n"

            if conversation_history:
                prompt = "\n".join(conversation_history) + f"\n\n{user_input}\n\n"

            # Generate a response using the pipeline
            outputs = pipe(prompt, **generation_args)

            # Decode the model output into text
            response = outputs[0]['generated_text'].strip()

            # Ensure the response is clean and concise
            response_lines = response.split("\n")
            response = "\n".join([line.strip() for line in response_lines if line.strip() != ""])

        print(f"\n{response}\n")

        # Update conversation history
        conversation_history.append(f"User: {user_input}")
        conversation_history.append(f"Assistant: {response}")

# Run the chatbot
chatbot()

