Post-Presentation Report 
Q1) Functionality of Masked Attention in Transformers
Transformer 모델에서 Masked Attention은 주로 시퀀스 데이터의 처리에서 중요한 역할을 합니다. 특히 언어 모델링과 같은 작업에서는 미래의 단어를 예측할 때 미래 정보를 참조하지 않도록 하는 것이 중요합니다. 이를 위해 Masked Attention이 사용됩니다.

1. 미래 정보 차단: Masked Attention은 시퀀스의 현재 위치 이후의 토큰을 마스킹하여, 모델이 현재 위치 이전의 토큰들만을 참조하도록 합니다. 이를 통해 언어 모델이 올바르게 학습될 수 있습니다.
2. 언어 모델링: 주로 트랜스포머 기반의 언어 모델(예: GPT-3)에서 사용되며, 문장의 다음 단어를 예측할 때 앞선 단어들만을 고려하도록 합니다.
3. 시퀀스-to-시퀀스 모델링: 번역 작업 등에서 소스 시퀀스와 타겟 시퀀스를 처리할 때 유용합니다. 타겟 시퀀스를 예측할 때 미래의 타겟 단어를 참조하지 않도록 합니다.


Q2) Reasons Why Some Features of Transformers  eare Superior to RNNs
1. 병렬 처리: Transformer는 전체 시퀀스를 한 번에 처리할 수 있어 병렬 처리가 가능합니다. 반면, RNN은 시퀀스를 한 단계씩 순차적으로 처리해야 합니다.
2. 장기 의존성 문제 해결: RNN은 시퀀스가 길어질수록 장기 의존성을 유지하기 어렵습니다. Transformer의 Self-Attention 메커니즘은 각 단어가 시퀀스 내 다른 모든 단어와 직접적으로 상호작용할 수 있게 하여 이 문제를 해결합니다. 멀리 떨어진 두 정보를 쉽게 얻어낼 수 있다는 점이 장점입니다. 
3. 학습 효율성: Transformer는 더 빠르게 학습할 수 있습니다. 이는 병렬 처리와 관련이 있으며, 더 깊은 네트워크를 효과적으로 학습할 수 있게 합니다.
4. 정보 손실 방지: Transformer는 Hidden State에 의존하지 않기 때문에 중요한 메모리를 잃지 않고 각 레이어에서 모든 정보를 유지할 수 있습니다.


Q3) Disadvantages of Transformers
1. 연산 비용: Self-Attention 메커니즘은 O(n^2)의 연산 복잡도를 가지며, 시퀀스 길이가 길어질수록 연산 비용이 크게 증가합니다.
2. 메모리 사용량: 모든 토큰 쌍 간의 상호작용을 계산하기 때문에 메모리 사용량이 많아집니다.
3. 데이터 요구량: 일반적으로 대규모 데이터셋에서 잘 작동하며, 소규모 데이터셋에서는 성능이 저하될 수 있습니다.
4. 설계 복잡성: 다양한 하이퍼파라미터와 아키텍처 설계가 필요하여, 최적의 모델을 찾는 과정이 복잡하고 시간이 많이 걸릴 수 있습니다.


Q4) Why some still use CNN
1. 이미지 처리: CNN은 이미지 인식 및 분류 작업에서 뛰어난 성능을 발휘합니다. 필터를 사용하여 지역적 특성을 추출하고, 이미지의 공간적 구조를 잘 유지할 수 있습니다.
2. 효율성: Convolution 연산은 효율적이며, 적은 파라미터로도 좋은 성능을 낼 수 있습니다. 이는 연산 비용과 메모리 사용량을 줄여줍니다.
3. 특화된 작업: CNN은 자연어 처리보다는 이미지나 비디오와 같은 시각적 데이터에서 더 나은 성능을 보이며, 해당 분야에서 여전히 많이 사용됩니다.
4. 전이 학습: 사전 학습된 CNN 모델은 다양한 컴퓨터 비전 작업에 전이 학습을 통해 쉽게 적용될 수 있습니다.
5. 데이터 양: 데이터 양이 방대하지않다면, CNN이 더 편리하게 사용될 수 있습니다. 


Q5) Difference between word and token
단어와 토큰은 자연어 처리에서의 기본 단위로, 서로 다른 정의와 역할을 가집니다. 

단어: 자연어에서의 기본 단위로, 공백이나 구두점으로 구분됩니다. 
토큰: 텍스트 데이터를 처리하기 위해 단어, 부분 단어, 심지어 문자까지 나눠진 단위입니다. 

예를 들어, "Hello world!"는 두 개의 단어로 구성됩니다.
토크나이저에 따라 다르게 나눠질 수 있으며, 언어 모델에서 입력으로 사용됩니다. 예를 들어, "Hello world!"는 토크나이저에 따라 ["Hello", "world", "!"] 또는 ["Hel", "lo", "wor", "ld", "!"]와 같이 나눠질 수 있습니다.
